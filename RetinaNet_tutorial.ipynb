{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet\n",
    "\n",
    "<br>\n",
    "\n",
    "### preparation\n",
    "\n",
    "- toy dataset: [FDDB: Face Detection Data Set and Benchmark](http://vis-www.cs.umass.edu/fddb/)\n",
    "\n",
    "- run the script(ellipsis_to_rectangle.py) to convert annotations in ellipsis to rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import imread, resize, imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import glob\n",
    "import json\n",
    "from datasets.utils import anchor_targets_bbox, bbox_transform, anchors_for_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_EXTENSIONS = ['png', 'jpg', 'bmp']\n",
    "\n",
    "def load_json(json_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_best_anchor(anchors, box_wh):\n",
    "    \"\"\"\n",
    "    Select the best anchor with highest IOU\n",
    "    \"\"\"\n",
    "    box_wh = np.array(box_wh)\n",
    "    best_iou = 0\n",
    "    best_anchor = 0\n",
    "    for k, anchor in enumerate(anchors):\n",
    "        intersect_wh = np.maximum(np.minimum(box_wh, anchor), 0.0)\n",
    "        intersect_area = intersect_wh[0] * intersect_wh[1]\n",
    "        box_area = box_wh[0] * box_wh[1]\n",
    "        anchor_area = anchor[0] * anchor[1]\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_anchor = k\n",
    "    return best_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_dir, image_size, no_label=False):\n",
    "    \"\"\"\n",
    "    Load the data and preprocessing for RetinaNet detector\n",
    "    :param data_dir: str, path to the directory to read.\n",
    "                     It should include class_map, annotations\n",
    "    :image_size: tuple, image size for resizing images\n",
    "    :no_label: bool, whetehr to load labels\n",
    "    :return: X_set: np.ndarray, shape: (N, H, W, C).\n",
    "             y_set: np.ndarray, shape: (N, N_box, 5+num_classes).\n",
    "    \"\"\"\n",
    "    im_dir = os.path.join(data_dir, 'images')\n",
    "    class_map_path = os.path.join(data_dir, 'classes.json')\n",
    "    class_map = load_json(class_map_path)\n",
    "    anchors = anchors_for_shape(image_size)\n",
    "    num_classes = len(class_map)\n",
    "    ih, iw = image_size\n",
    "    im_paths = []\n",
    "    for ext in IM_EXTENSIONS:\n",
    "        im_paths.extend(glob.glob(os.path.join(im_dir, '*.{}'.format(ext))))\n",
    "    anno_dir = os.path.join(data_dir, 'annotations')\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for im_path in im_paths:\n",
    "        # load image and resize image\n",
    "        im = imread(im_path)\n",
    "        im = np.array(im, dtype=np.float32)\n",
    "        im_original_sizes = im.shape[:2]\n",
    "        im = resize(im, (image_size[1], image_size[0]))\n",
    "        if len(im.shape) == 2:\n",
    "            im = np.expand_dims(im, 2)\n",
    "            im = np.concatenate([im, im, im], -1)\n",
    "        images.append(im)\n",
    "\n",
    "        if no_label:\n",
    "            labels.append(0)\n",
    "            continue\n",
    "        # load bboxes and reshape for retina model\n",
    "        name = os.path.splitext(os.path.basename(im_path))[0]\n",
    "        anno_path = os.path.join(anno_dir, '{}.anno'.format(name))\n",
    "        anno = load_json(anno_path)\n",
    "        bboxes = []\n",
    "\n",
    "        for c_idx, c_name in class_map.items():\n",
    "            if c_name not in anno:\n",
    "                continue\n",
    "            for x_min, y_min, x_max, y_max in anno[c_name]:\n",
    "                oh, ow = im_original_sizes\n",
    "                x_min, y_min, x_max, y_max = x_min / ow, y_min / oh, x_max/ ow, y_max / oh\n",
    "                bboxes.append([x_min, y_min, x_max, y_max, int(c_idx)+1])\n",
    "            bboxes = np.array(bboxes)\n",
    "            bboxes = np.array([iw, ih, iw, ih, 1], dtype=np.float32) * bboxes\n",
    "\n",
    "            b_labels, annotations = anchor_targets_bbox(im.shape, bboxes, num_classes + 1, anchors)\n",
    "            regression = bbox_transform(anchors, annotations)\n",
    "            label = np.array(np.append(regression, b_labels, axis=1), dtype=np.float32)\n",
    "        labels.append(label)\n",
    "\n",
    "    X_set = np.array(images, dtype=np.float32)\n",
    "    y_set = np.array(labels, dtype=np.float32)\n",
    "\n",
    "    return X_set, y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, images, labels=None):\n",
    "        \"\"\"\n",
    "        Construct a new DataSet object.\n",
    "        :param images: np.ndarray, shape: (N, H, W, C)\n",
    "        :param labels: np.ndarray, shape: (N, g_H, g_W, anchors, 5 + num_classes).\n",
    "        \"\"\"\n",
    "        if labels is not None:\n",
    "            assert images.shape[0] == labels.shape[0],\\\n",
    "                ('Number of examples mismatch, between images and labels')\n",
    "        self._num_examples = images.shape[0]\n",
    "        self._images = images\n",
    "        self._labels = labels  # NOTE: this can be None, if not given.\n",
    "        # image/label indices(can be permuted)\n",
    "        self._indices = np.arange(self._num_examples, dtype=np.uint)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset some variables.\"\"\"\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    def sample_batch(self, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Return sample examples from this dataset.\n",
    "        :param batch_size: int, size of a sample batch.\n",
    "        :param shuffle: bool, whether to shuffle the whole set while sampling a batch.\n",
    "        :return: batch_images: np.ndarray, shape: (N, H, W, C)\n",
    "                 batch_labels: np.ndarray, shape: (N, g_H, g_W, anchors, 5 + num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        if shuffle:\n",
    "            indices = np.random.choice(self._num_examples, batch_size)\n",
    "        else:\n",
    "            indices = np.arange(batch_size)\n",
    "        batch_images = self._images[indices]\n",
    "        if self._labels is not None:\n",
    "            batch_labels = self._labels[indices]\n",
    "        else:\n",
    "            batch_labels = None\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def next_batch(self, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Return the next 'batch_size' examples from this dataset.\n",
    "        :param batch_size: int, size of a single batch.\n",
    "        :param shuffle: bool, whether to shuffle the whole set while sampling a batch.\n",
    "        :return: batch_images: np.ndarray, shape: (N, H, W, C)\n",
    "                 batch_labels: np.ndarray, shape: (N, g_H, g_W, anchors, 5 + num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        start_index = self._index_in_epoch\n",
    "\n",
    "        # Shuffle the dataset, for the first epoch\n",
    "        if self._epochs_completed == 0 and start_index == 0 and shuffle:\n",
    "            np.random.shuffle(self._indices)\n",
    "\n",
    "        # Go to the next epoch, if current index goes beyond the total number\n",
    "        # of examples\n",
    "        if start_index + batch_size > self._num_examples:\n",
    "            # Increment the number of epochs completed\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start_index\n",
    "            indices_rest_part = self._indices[start_index:self._num_examples]\n",
    "\n",
    "            # Shuffle the dataset, after finishing a single epoch\n",
    "            if shuffle:\n",
    "                np.random.shuffle(self._indices)\n",
    "\n",
    "            # Start the next epoch\n",
    "            start_index = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end_index = self._index_in_epoch\n",
    "            indices_new_part = self._indices[start_index:end_index]\n",
    "\n",
    "            images_rest_part = self._images[indices_rest_part]\n",
    "            images_new_part = self._images[indices_new_part]\n",
    "            batch_images = np.concatenate(\n",
    "                (images_rest_part, images_new_part), axis=0)\n",
    "            if self._labels is not None:\n",
    "                labels_rest_part = self._labels[indices_rest_part]\n",
    "                labels_new_part = self._labels[indices_new_part]\n",
    "                batch_labels = np.concatenate(\n",
    "                    (labels_rest_part, labels_new_part), axis=0)\n",
    "            else:\n",
    "                batch_labels = None\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end_index = self._index_in_epoch\n",
    "            indices = self._indices[start_index:end_index]\n",
    "            batch_images = self._images[indices]\n",
    "            if self._labels is not None:\n",
    "                batch_labels = self._labels[indices]\n",
    "            else:\n",
    "                batch_labels = None\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "root_dir = os.path.join('data\\face\\\\')\n",
    "trainval_dir = os.path.join(root_dir, 'train')\n",
    "test_dir = os.path.join(root_dir, 'test')\n",
    "class_map = load_json(os.path.join(trainval_dir, 'classes.json'))\n",
    "\n",
    "# set hyperparameters for data\n",
    "IM_SIZE = (512, 512)\n",
    "NUM_CLASSES = 1\n",
    "VALID_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, y_trainval = read_data(trainval_dir, IM_SIZE)\n",
    "trainval_size = X_trainval.shape[0]\n",
    "val_size = int(trainval_size * VALID_RATIO)\n",
    "val_set = DataSet(X_trainval[:val_size], y_trainval[:val_size])\n",
    "train_set = DataSet(X_trainval[val_size:], y_trainval[val_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = read_data(test_dir, IM_SIZE)\n",
    "test_set = DataSet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-51a1715bf056>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresnet_v2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresnet_v2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mslim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\contrib\\distributions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftplus_inverse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtridiag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf_normal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\contrib\\distributions\\python\\ops\\estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_compute_weighted_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_RegressionHead\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\contrib\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\basic_session_run_hooks.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;34m'tf.contrib.learn.basic_session_run_hooks.LoggingTensorHook'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;34m'tf.train.LoggingTensorHook'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     basic_session_run_hooks.LoggingTensorHook)\n\u001b[0m\u001b[0;32m     34\u001b[0m StopAtStepHook = deprecated_alias(\n\u001b[0;32m     35\u001b[0m     \u001b[1;34m'tf.contrib.learn.basic_session_run_hooks.StopAtStepHook'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mdeprecated_alias\u001b[1;34m(deprecated_name, name, func_or_class, warn_once)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;31m# Make a new class with __init__ wrapped in a warning.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[1;32mclass\u001b[0m \u001b[0m_NewClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m       __doc__ = decorator_utils.add_notice_to_docstring(\n\u001b[0;32m    170\u001b[0m           \u001b[0mfunc_or_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Please use %s instead.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36m_NewClass\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m                            'It will be removed in a future version. '])\n\u001b[0;32m    174\u001b[0m       \u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_or_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m       \u001b[0m__module__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m       \u001b[1;33m@\u001b[0m\u001b[0m_wrap_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36m_call_location\u001b[1;34m(outer)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m   \u001b[1;34m\"\"\"Returns call location given level up from current call.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m   \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# CPython internals are available, use them for performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m   \u001b[1;34m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\inspect.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1468\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[1;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1470\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m         \u001b[0mframeinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m             \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source code not available'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv3.5\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Update the filename to module name cache and check yet again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[1;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__file__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.nets import resnet_v2 as resnet_v2\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, filters, kernel_size, strides, padding='SAME', use_bias=True):\n",
    "    return tf.layers.conv2d(x, filters, kernel_size, strides, padding, use_bias=use_bias)\n",
    "\n",
    "def build_head_loc(x, num_anchors, depth=4, name='head_loc'):\n",
    "    head = x\n",
    "    with tf.variable_scope(name):\n",
    "        for _ in range(depth):\n",
    "            head = tf.nn.relu(conv_layer(head, 256, (3, 3), (1, 1)))\n",
    "        output_channels = num_anchors * 4\n",
    "        head = conv_layer(head, output_channels, (3, 3), (1, 1))\n",
    "    return head\n",
    "\n",
    "def build_head_cls(x, num_anchors, num_classes, depth=4, prior_probs=0.01, name='head_cls'):\n",
    "    head = x\n",
    "    with tf.variable_scope(name):\n",
    "        for _ in range(depth):\n",
    "            head = tf.nn.relu(conv_layer(head, 256, (3, 3), (1, 1)))\n",
    "        output_channels = num_anchors * num_classes\n",
    "        bias = np.zeros((num_classes, 1, 1), dtype=np.float32)\n",
    "        bias[0] = np.log((num_classes - 1) * (1 - prior_probs) / (prior_probs))\n",
    "        bias = np.vstack([bias for _ in range(num_anchors)])\n",
    "        biases = tf.get_variable('biases', [num_anchors * num_classes], tf.float32,\\\n",
    "                                tf.constant_initializer(value=bias))\n",
    "        head = conv_layer(head, output_channels, (3, 3), (1, 1), use_bias=False) + biases\n",
    "    return head\n",
    "\n",
    "def resize_to_target(x, target):\n",
    "    size = (tf.shape(target)[1], tf.shape(target)[2])\n",
    "    x = tf.image.resize_bilinear(x, size)\n",
    "    return tf.cast(x, x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = list(IM_SIZE) + [3]\n",
    "num_classes = NUM_CLASSES\n",
    "anchors = anchors_for_shape(input_shape[:2])\n",
    "num_anchors = 9\n",
    "\n",
    "# Prepare Input\n",
    "X = tf.placeholder(tf.float32, [None] + input_shape)\n",
    "y = tf.placeholder(tf.float32, [None] + [anchors.shape[0]] + [5 + num_classes])\n",
    "is_train = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init build graph\n",
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "frontend = 'resnet_v2_50'\n",
    "\n",
    "with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
    "    frontend_dir = os.path.join('pretrained_models', '{}.ckpt'.format(frontend))\n",
    "    logits, end_points = resnet_v2.resnet_v2_50(X, is_training=is_train)\n",
    "    d['init_fn'] = slim.assign_from_checkpoint_fn(model_path=frontend_dir,\n",
    "                                              var_list=slim.get_model_variables(frontend))\n",
    "convs = [end_points[frontend + '/block{}'.format(x)] for x in [4, 2, 1]]\n",
    "\n",
    "with tf.variable_scope('layer5'):\n",
    "    d['s_5'] = conv_layer(convs[0], 256, (1, 1), (1, 1))\n",
    "    d['cls_head5'] = build_head_cls(d['s_5'], num_anchors, num_classes + 1)\n",
    "    d['loc_head5'] = build_head_loc(d['s_5'], num_anchors)\n",
    "    d['flat_cls_head5'] = tf.reshape(d['cls_head5'], (tf.shape(d['cls_head5'])[0], -1, num_classes + 1))\n",
    "    d['flat_loc_head5'] = tf.reshape(d['loc_head5'], (tf.shape(d['loc_head5'])[0], -1, 4))\n",
    "\n",
    "with tf.variable_scope('layer6'):\n",
    "    d['s_6'] = conv_layer(d['s_5'], 256, (3, 3), (2, 2))\n",
    "    d['cls_head6'] = build_head_cls(d['s_6'], num_anchors, num_classes + 1)\n",
    "    d['loc_head6'] = build_head_loc(d['s_6'], num_anchors)\n",
    "    d['flat_cls_head6'] = tf.reshape(d['cls_head6'], (tf.shape(d['cls_head6'])[0], -1, num_classes + 1))\n",
    "    d['flat_loc_head6'] = tf.reshape(d['loc_head6'], (tf.shape(d['loc_head6'])[0], -1, 4))\n",
    "\n",
    "with tf.variable_scope('layer7'):\n",
    "    d['s_7'] = conv_layer(tf.nn.relu(d['s_6']), 256, (3, 3), (2, 2))\n",
    "    d['cls_head7'] = build_head_cls(d['s_7'], num_anchors, num_classes + 1)\n",
    "    d['loc_head7'] = build_head_loc(d['s_7'], num_anchors)\n",
    "    d['flat_cls_head7'] = tf.reshape(d['cls_head7'], (tf.shape(d['cls_head7'])[0], -1, num_classes + 1))\n",
    "    d['flat_loc_head7'] = tf.reshape(d['loc_head7'], (tf.shape(d['loc_head7'])[0], -1, 4))\n",
    "\n",
    "with tf.variable_scope('layer4'):\n",
    "    d['up4'] = resize_to_target(d['s_5'], convs[1])\n",
    "    d['s_4'] = conv_layer(convs[1], 256, (1, 1), (1, 1)) + d['up4']\n",
    "    d['cls_head4'] = build_head_cls(d['s_4'], num_anchors, num_classes + 1)\n",
    "    d['loc_head4'] = build_head_loc(d['s_4'], num_anchors)\n",
    "    d['flat_cls_head4'] = tf.reshape(d['cls_head4'], (tf.shape(d['cls_head4'])[0], -1, num_classes + 1))\n",
    "    d['flat_loc_head4'] = tf.reshape(d['loc_head4'], (tf.shape(d['loc_head4'])[0], -1, 4))\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    d['up3'] = resize_to_target(d['s_4'], convs[2])\n",
    "    d['s_3'] = conv_layer(convs[2], 256, (1, 1), (1, 1)) + d['up3']\n",
    "    d['cls_head3'] = build_head_cls(d['s_3'], num_anchors, num_classes + 1)\n",
    "    d['loc_head3'] = build_head_loc(d['s_3'], num_anchors)\n",
    "    d['flat_cls_head3'] = tf.reshape(d['cls_head3'], (tf.shape(d['cls_head3'])[0], -1, num_classes + 1))\n",
    "    d['flat_loc_head3'] = tf.reshape(d['loc_head3'], (tf.shape(d['loc_head3'])[0], -1, 4))\n",
    "\n",
    "with tf.variable_scope('head'):\n",
    "    d['cls_head'] = tf.concat((d['flat_cls_head3'],\n",
    "                               d['flat_cls_head4'],\n",
    "                               d['flat_cls_head5'],\n",
    "                               d['flat_cls_head6'],\n",
    "                               d['flat_cls_head7']), axis=1)\n",
    "\n",
    "    d['loc_head'] = tf.concat((d['flat_loc_head3'],\n",
    "                               d['flat_loc_head4'],\n",
    "                               d['flat_loc_head5'],\n",
    "                               d['flat_loc_head6'],\n",
    "                               d['flat_loc_head7']), axis=1)\n",
    "\n",
    "    d['logits'] = tf.concat((d['loc_head'], d['cls_head']), axis=2)\n",
    "    d['pred'] = tf.concat((d['loc_head'], tf.nn.softmax(d['cls_head'], axis=-1)), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = d['logits']\n",
    "pred = d['pred']\n",
    "pred_y = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_pred, y_true, alpha=0.25, gamma=2.0):\n",
    "    with tf.variable_scope('focal_loss'):\n",
    "        y_true, y_pred = [x[:, :, 4:] for x in [y_true, y_pred]]\n",
    "\n",
    "        total_state = tf.reduce_max(y_true, axis=-1, keepdims=True)\n",
    "        total_state = tf.cast(tf.math.equal(total_state, 1), dtype=tf.float32)\n",
    "        pos_state = tf.reduce_max(y_true[..., 1:], axis=-1, keepdims=True)\n",
    "        pos_state = tf.cast(tf.math.equal(pos_state, 1), dtype=tf.float32)\n",
    "        divisor = tf.reduce_sum(pos_state)\n",
    "        divisor = tf.clip_by_value(divisor, 1, divisor)\n",
    "\n",
    "        labels = tf.multiply(total_state, y_true)\n",
    "        class_logits = tf.multiply(total_state, y_pred)\n",
    "        class_probs = tf.nn.softmax(class_logits, axis=-1)\n",
    "        focal_weight = alpha * tf.pow((1-class_probs), gamma)\n",
    "        mask_focal_weight = tf.multiply(labels, focal_weight)\n",
    "        mask_focal_weight = tf.reduce_max(mask_focal_weight, axis=-1, keepdims=True)\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=class_logits)\n",
    "        entropy = tf.expand_dims(entropy, axis=-1)\n",
    "        focal_loss = mask_focal_weight * entropy\n",
    "        focal_loss = tf.reduce_sum((focal_loss / divisor))\n",
    "    return focal_loss\n",
    "\n",
    "def smooth_l1_loss(y_pred, y_true, sigma=3.0):\n",
    "    sigma2 = sigma * sigma\n",
    "    with tf.variable_scope('smooth_l1_loss'):\n",
    "        anchor_state = tf.reduce_max(y_true[:, :, 5:], axis=-1, keepdims=True)\n",
    "        y_true, y_pred = [x[:, :, :4] for x in [y_true, y_pred]]\n",
    "\n",
    "        regression = y_pred\n",
    "        regression_target = y_true[:, :, :4]\n",
    "        pos_state = tf.cast(tf.math.equal(anchor_state, 1), dtype=tf.float32)\n",
    "        divisor = tf.reduce_sum(pos_state)\n",
    "        divisor = tf.clip_by_value(divisor, 1, divisor)\n",
    "\n",
    "        abs_loss = tf.abs(tf.multiply(pos_state, (regression-regression_target)))\n",
    "\n",
    "        smooth_l1_sign = tf.cast(tf.less(abs_loss, 1.0/sigma2), dtype=tf.float32)\n",
    "        smooth_l1_option1 = tf.multiply(tf.pow(abs_loss, 2), 0.5*sigma2)\n",
    "        smooth_l1_option2 = abs_loss - (0.5/sigma2)\n",
    "        smooth_l1_results = tf.multiply(smooth_l1_option1, smooth_l1_sign) + \\\n",
    "                            tf.multiply(smooth_l1_option2, (1 - smooth_l1_sign))\n",
    "        smooth_l1_results = tf.reduce_sum((smooth_l1_results / divisor))\n",
    "    return smooth_l1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_alpha = 1\n",
    "with tf.variable_scope('losses'):\n",
    "    conf_loss = focal_loss(logits, y)\n",
    "    regress_loss = smooth_l1_loss(logits, y)\n",
    "    total_loss = conf_loss + r_alpha * regress_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.utils import get_boxes, cal_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, y_pred, anchors):\n",
    "    \"\"\"Compute Recall for a given predicted bboxes\"\"\"\n",
    "    bboxes = get_boxes(y_pred, anchors)\n",
    "    gt_bboxes = get_boxes(y_true, anchors)\n",
    "    score = cal_recall(gt_bboxes, bboxes)\n",
    "    return score\n",
    "\n",
    "def predict(sess, dataset, **kwargs):\n",
    "    batch_size = kwargs.pop('batch_size', 16)\n",
    "    pred_size = dataset.num_examples\n",
    "    num_steps = pred_size // batch_size\n",
    "    flag = int(bool(pred_size % batch_size))\n",
    "    # Start prediction loop\n",
    "    _y_pred = []\n",
    "    for i in range(num_steps + flag):\n",
    "        if i == num_steps and flag:\n",
    "            _batch_size = pred_size - num_steps * batch_size\n",
    "        else:\n",
    "            _batch_size = batch_size\n",
    "        X_true, _ = dataset.next_batch(_batch_size, shuffle=False)\n",
    "\n",
    "        # Compute predictions\n",
    "        y_pred = sess.run(pred_y, feed_dict={\n",
    "                          X: X_true, is_train: False})\n",
    "\n",
    "        _y_pred.append(y_pred)\n",
    "    _y_pred = np.concatenate(_y_pred, axis=0)\n",
    "    return _y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=graph, config=config)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for training\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "eps = 1e-3\n",
    "num_eval = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "update_vars = tf.trainable_variables()\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).\\\n",
    "                minimize(total_loss, var_list=update_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "d['init_fn'](sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_set.num_examples\n",
    "num_steps_per_epoch = train_size // batch_size\n",
    "num_steps = num_epochs * num_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curr_epoch = 1\n",
    "best_score = 0\n",
    "curr_score = 0\n",
    "# Start training loop\n",
    "for i in range(num_steps):\n",
    "    X_true, y_true = train_set.next_batch(batch_size, shuffle=True)\n",
    "    _, loss, y_pred = sess.run([train_op, total_loss, pred_y],\n",
    "                              feed_dict={X:X_true, y: y_true, is_train: True})\n",
    "    if (i+1) % num_eval == 0:\n",
    "        step_score = score(y_true, y_pred, anchors)\n",
    "        eval_y_pred = predict(sess, val_set)\n",
    "        eval_score = score(val_set.labels, eval_y_pred, anchors)\n",
    "        print('[epoch {}]\\tloss: {:.6f} |Train score: {:.6f} |Eval score: {:.6f}'\n",
    "      .format(curr_epoch, loss, step_score, eval_score))\n",
    "        curr_score = eval_score\n",
    "\n",
    "    if curr_score > best_score + eps:\n",
    "        best_score = curr_score\n",
    "        saver.save(sess, './retina.ckpt')\n",
    "        \n",
    "    if (i+1) % num_steps_per_epoch == 0:\n",
    "        curr_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test image and draw bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import draw_pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, './retina.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict(sess, test_set)\n",
    "test_score = score(test_set.labels, test_y_pred, anchors)\n",
    "\n",
    "print('Test performance: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_idx = np.random.choice(test_set.num_examples, 1)\n",
    "test_images = test_set.images[r_idx]\n",
    "test_pred_y = sess.run(pred_y, feed_dict={X: test_images, is_train: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = get_boxes(test_pred_y[0], anchors)\n",
    "bboxes = bboxes[np.nonzero(np.any(bboxes > 0, axis=1))]\n",
    "boxed_img = draw_pred_boxes(test_images[0], bboxes, class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "imshow(cv2.cvtColor(boxed_img, cv2.COLOR_BGR2RGB)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
